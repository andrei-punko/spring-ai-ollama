spring.application.name=spring-ai-ollama
spring.threads.virtual.enabled=true
server.port=8090

# We use docker compose to run the Ollama docker instance (see docker-compose.yml)
spring.docker.compose.enabled=true

# Use local Ollama
spring.ai.ollama.base-url=${OLLAMA_BASE_URL:http://localhost:11434}

# Default Ollama model can be changed by setting the below property.
# gemma3 (1b/4b) - The current, most capable model that runs on a single GPU
# llama3.1 (8b) - new state-of-the-art model from Meta
# qwen2.5-coder (3b/7b) - the latest series of Code-Specific Qwen models, with significant improvements in code generation, code reasoning, and code fixing
# starcoder2 (3b/7b) - model by BigCode+NVIDIA for code-generation tasks
spring.ai.ollama.chat.model=${OLLAMA_MODEL_NAME:tinyllama}

# The temperature of the model. Increasing the temperature will make the model answer more creatively.
spring.ai.ollama.chat.options.temperature=0.5

# Auto-pulling Models
spring.ai.ollama.init.pull-model-strategy=when_missing
spring.ai.ollama.init.timeout=1m
spring.ai.ollama.init.max-retries=3
