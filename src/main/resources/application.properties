spring.application.name=ai-chat-ollama
spring.threads.virtual.enabled=true
server.port=8090

# Use local Ollama
spring.ai.ollama.base-url=http://localhost:11434

# The temperature of the model. Increasing the temperature will make the model answer more creatively.
spring.ai.ollama.chat.options.temperature=0.5

# Auto-pulling Models
spring.ai.ollama.init.pull-model-strategy=when_missing
spring.ai.ollama.init.timeout=1m
spring.ai.ollama.init.max-retries=3

# Default Ollama model can be changed by setting the below property.
# gemma3 (1b/4b) - The current, most capable model that runs on a single GPU
# llama3.1 (8b) - new state-of-the-art model from Meta
# qwen2.5-coder (3b/7b) - the latest series of Code-Specific Qwen models, with significant improvements in code generation, code reasoning, and code fixing
# starcoder2 (3b/7b) - model by BigCode+NVIDIA for code-generation tasks
spring.ai.ollama.chat.model=gemma3:1b
