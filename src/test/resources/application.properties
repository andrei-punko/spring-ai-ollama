# If running the Ollama Docker Instance separately, then set this property
spring.docker.compose.enabled=false

spring.ai.ollama.chat.model=tinyllama
